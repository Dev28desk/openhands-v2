#!/bin/bash
set -e

echo "üöÄ Complete System Deployment Script for 154.201.127.161"
echo "========================================================"
echo "Installing: OpenHands + Open WebUI + Ollama + CodeLlama 70B"
echo "Ubuntu 24.04 LTS Compatible Version"
echo ""

# Variables
SERVER_IP="154.201.127.161"
OLLAMA_MODEL="codellama:70b-code"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1"
}

print_error() {
    echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1"
}

# Fix broken repositories
print_status "üîß Fixing APT repositories..."
sudo rm -f /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo rm -f /etc/apt/sources.list.d/nvidia-docker.list
sudo rm -f /etc/apt/sources.list.d/nvidia*
sudo rm -f /etc/apt/sources.list.d/cuda*

# Clean APT cache
sudo apt-get clean
sudo rm -rf /var/lib/apt/lists/*

# Clean up everything
print_status "üßπ Cleaning up existing installations..."
sudo systemctl stop docker ollama nginx apache2 2>/dev/null || true
sudo systemctl disable docker ollama nginx apache2 2>/dev/null || true
sudo apt-get remove -y docker docker-engine docker.io containerd runc nginx apache2 2>/dev/null || true
sudo apt-get autoremove -y 2>/dev/null || true
sudo rm -rf /opt/deskdev /opt/openhands /opt/open-webui /var/lib/docker /etc/docker
sudo rm -rf /usr/local/bin/ollama /etc/systemd/system/ollama.service
sudo rm -rf /etc/nginx /etc/apache2
sudo docker stop $(sudo docker ps -aq) 2>/dev/null || true
sudo docker rm $(sudo docker ps -aq) 2>/dev/null || true
sudo docker rmi $(sudo docker images -q) 2>/dev/null || true

# Update system with fixed repositories
print_status "üì¶ Updating system packages..."
sudo apt-get update -y || {
    print_warning "Initial update failed, cleaning sources..."
    sudo rm -rf /var/lib/apt/lists/*
    sudo apt-get update -y
}
sudo apt-get upgrade -y
sudo apt-get install -y curl wget git nano htop net-tools ufw ca-certificates gnupg lsb-release software-properties-common

# Configure firewall
print_status "üî• Configuring firewall..."
sudo ufw --force disable
sudo ufw --force reset
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow 22/tcp    # SSH
sudo ufw allow 80/tcp    # HTTP
sudo ufw allow 443/tcp   # HTTPS
sudo ufw allow 3000/tcp  # OpenHands Frontend
sudo ufw allow 3001/tcp  # OpenHands Backend
sudo ufw allow 8080/tcp  # Open WebUI
sudo ufw allow 11434/tcp # Ollama API
sudo ufw allow 30369/tcp # OpenHands Runtime
sudo ufw --force enable

# Install Docker (Ubuntu 24.04 specific)
print_status "üê≥ Installing Docker..."
# Remove any old Docker GPG keys
sudo rm -f /usr/share/keyrings/docker-archive-keyring.gpg
sudo rm -f /etc/apt/keyrings/docker.gpg

# Add Docker's official GPG key
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Update and install Docker
sudo apt-get update -y
sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# Add user to docker group
sudo usermod -aG docker $USER || true

# Start Docker
sudo systemctl enable docker
sudo systemctl start docker

# Verify Docker installation
if ! sudo docker run hello-world >/dev/null 2>&1; then
    print_error "Docker installation failed!"
    exit 1
fi
print_status "‚úÖ Docker installed successfully"

# Install Docker Compose
print_status "üê≥ Installing Docker Compose..."
sudo curl -L "https://github.com/docker/compose/releases/download/v2.24.6/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# Verify Docker Compose
if ! docker-compose version >/dev/null 2>&1; then
    print_error "Docker Compose installation failed!"
    exit 1
fi

# Install Ollama
print_status "ü§ñ Installing Ollama..."
curl -fsSL https://ollama.ai/install.sh | sh

# Wait for Ollama to be installed
sleep 5

# Configure Ollama
print_status "‚öôÔ∏è Configuring Ollama..."
sudo mkdir -p /etc/systemd/system
sudo tee /etc/systemd/system/ollama.service > /dev/null << 'EOF'
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
Type=simple
ExecStart=/usr/local/bin/ollama serve
User=root
Group=root
Restart=always
RestartSec=3
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_ORIGINS=*"
Environment="OLLAMA_CORS_ALLOWED_ORIGINS=*"
Environment="OLLAMA_KEEP_ALIVE=24h"
Environment="OLLAMA_MODELS=/usr/share/ollama/.ollama/models"

[Install]
WantedBy=default.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl restart ollama

# Wait for Ollama to start
print_status "‚è≥ Waiting for Ollama to start..."
for i in {1..30}; do
    if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
        print_status "‚úÖ Ollama is running"
        break
    fi
    sleep 2
done

# Test Ollama
if ! curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
    print_error "Ollama is not responding!"
    sudo journalctl -u ollama -n 50
    exit 1
fi

# Pull CodeLlama 70B model
print_status "üì• Pulling CodeLlama 70B model (this will take 30-60 minutes)..."
print_warning "The 70B model requires ~40GB of disk space and ~70GB of RAM to run efficiently"
ollama pull $OLLAMA_MODEL || {
    print_error "Failed to pull model! Trying again..."
    sleep 10
    ollama pull $OLLAMA_MODEL
}

# Create application directories
print_status "üìÅ Creating application directories..."
sudo mkdir -p /opt/{deskdev,open-webui}/{data,config}
sudo mkdir -p /opt/deskdev/{workspace,custom/templates,custom/static/{css,images}}
sudo chmod -R 755 /opt

# Create landing page
print_status "üé® Creating landing page..."
sudo tee /opt/deskdev/custom/templates/index.html > /dev/null << 'EOF'
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Development Platform - 154.201.127.161</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #0a0a0a;
            color: #ffffff;
            min-height: 100vh;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        h1 {
            font-size: 3rem;
            margin-bottom: 2rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .services {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin-top: 3rem;
        }
        .service-card {
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 1rem;
            padding: 2rem;
            transition: transform 0.2s;
        }
        .service-card:hover {
            transform: translateY(-5px);
            background: rgba(255, 255, 255, 0.08);
        }
        .service-card h2 {
            margin-bottom: 1rem;
            color: #667eea;
        }
        .service-card p {
            color: #999;
            margin-bottom: 1.5rem;
        }
        .btn {
            display: inline-block;
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 0.5rem;
            font-weight: 600;
            transition: opacity 0.2s;
        }
        .btn:hover {
            opacity: 0.9;
        }
        .status {
            margin-top: 3rem;
            padding: 1rem;
            background: rgba(0, 255, 0, 0.1);
            border: 1px solid rgba(0, 255, 0, 0.3);
            border-radius: 0.5rem;
        }
        .model-info {
            margin-top: 2rem;
            padding: 1rem;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 0.5rem;
        }
        code {
            background: rgba(255, 255, 255, 0.1);
            padding: 0.25rem 0.5rem;
            border-radius: 0.25rem;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>AI Development Platform</h1>
        <p style="font-size: 1.25rem; color: #999;">Complete AI development environment with CodeLlama 70B</p>
        
        <div class="services">
            <div class="service-card">
                <h2>üöÄ OpenHands</h2>
                <p>AI-powered software development assistant for building applications</p>
                <a href="http://154.201.127.161:3000" class="btn">Launch OpenHands</a>
            </div>
            
            <div class="service-card">
                <h2>üí¨ Open WebUI</h2>
                <p>Chat interface for interacting with CodeLlama 70B model</p>
                <a href="http://154.201.127.161:8080" class="btn">Launch Open WebUI</a>
            </div>
            
            <div class="service-card">
                <h2>ü§ñ Ollama API</h2>
                <p>Direct API access to CodeLlama and other models</p>
                <a href="http://154.201.127.161:11434" class="btn">API Endpoint</a>
            </div>
        </div>
        
        <div class="status">
            <h3>‚úÖ All Services Running</h3>
            <p>Server IP: <code>154.201.127.161</code></p>
        </div>
        
        <div class="model-info">
            <h3>üß† Available Model</h3>
            <p>Model: <code>codellama:70b-code</code></p>
            <p>Specialized for code generation, debugging, and software development tasks</p>
        </div>
    </div>
</body>
</html>
EOF

# Create main Docker Compose file
print_status "üê≥ Creating Docker Compose configuration..."
sudo tee /opt/docker-compose.yml > /dev/null << 'EOF'
version: '3.8'

networks:
  ai-network:
    driver: bridge

services:
  # OpenHands
  openhands:
    image: ghcr.io/all-hands-ai/openhands:latest
    container_name: openhands
    restart: unless-stopped
    user: root
    networks:
      - ai-network
    environment:
      # Core settings
      - SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.47-nikolaik
      - WORKSPACE_MOUNT_PATH=/opt/workspace_base
      
      # Ollama Configuration
      - LLM_BASE_URL=http://host.docker.internal:11434
      - LLM_MODEL=codellama:70b-code
      - LLM_API_KEY=
      - LLM_API_VERSION=v1
      - LLM_PROVIDER=ollama
      - DEFAULT_LLM_PROVIDER=ollama
      - DEFAULT_LLM_MODEL=codellama:70b-code
      
      # App Configuration
      - APP_NAME=OpenHands
      - FRONTEND_URL=http://154.201.127.161:3000
      - BACKEND_URL=http://154.201.127.161:3001
      
      # Runtime Configuration
      - RUNTIME_HOST=0.0.0.0
      - RUNTIME_PORT=30369
      - RUNTIME_TIMEOUT=300
      
      # Disable analytics
      - POSTHOG_CLIENT_KEY=
      - DISABLE_ANALYTICS=true
      
    ports:
      - "3000:3000"
      - "3001:3001"
      - "30369:30369"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/deskdev/data:/.openhands
      - /opt/deskdev/workspace:/opt/workspace_base

  # Open WebUI
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    networks:
      - ai-network
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - WEBUI_SECRET_KEY=your-secret-key-change-this-$(openssl rand -hex 32)
      - WEBUI_NAME=Open WebUI - CodeLlama 70B
      - ENABLE_SIGNUP=true
      - DEFAULT_MODELS=codellama:70b-code
      - WEBUI_URL=http://154.201.127.161:8080
    ports:
      - "8080:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - /opt/open-webui/data:/app/backend/data

  # Nginx reverse proxy
  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: unless-stopped
    networks:
      - ai-network
    ports:
      - "80:80"
    volumes:
      - /opt/nginx.conf:/etc/nginx/nginx.conf:ro
      - /opt/deskdev/custom/templates:/usr/share/nginx/html:ro
EOF

# Create Nginx configuration
print_status "üîß Creating Nginx configuration..."
sudo tee /opt/nginx.conf > /dev/null << 'EOF'
events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    # Increase timeouts for AI operations
    proxy_read_timeout 300s;
    proxy_connect_timeout 300s;
    proxy_send_timeout 300s;
    client_max_body_size 100M;
    
    # Main landing page
    server {
        listen 80;
        server_name _;
        
        location / {
            root /usr/share/nginx/html;
            index index.html;
            try_files $uri $uri/ /index.html;
        }
        
        # Proxy to OpenHands
        location /openhands {
            return 301 http://$server_addr:3000;
        }
        
        # Proxy to Open WebUI
        location /chat {
            return 301 http://$server_addr:8080;
        }
    }
}
EOF

# Start all services
print_status "üöÄ Starting all services..."
cd /opt
sudo docker-compose up -d

# Wait for services to start
print_status "‚è≥ Waiting for services to start..."
sleep 30

# Health checks
print_status "üè• Running health checks..."
echo ""
echo "Checking Ollama..."
if curl -s http://localhost:11434/api/tags | grep -q "codellama:70b-code"; then
    echo "‚úÖ Ollama: OK - CodeLlama 70B model loaded"
else
    echo "‚ùå Ollama: Model not found (may still be downloading)"
fi

echo "Checking OpenHands..."
curl -s http://localhost:3000 > /dev/null && echo "‚úÖ OpenHands: OK" || echo "‚ùå OpenHands: Failed"

echo "Checking Open WebUI..."
curl -s http://localhost:8080 > /dev/null && echo "‚úÖ Open WebUI: OK" || echo "‚ùå Open WebUI: Failed"

echo "Checking Nginx..."
curl -s http://localhost > /dev/null && echo "‚úÖ Nginx: OK" || echo "‚ùå Nginx: Failed"

# Display final information
print_status "‚úÖ Deployment Complete!"
echo ""
echo "=========================================="
echo "üåê Access your services:"
echo "=========================================="
echo "üìç Landing Page:    http://$SERVER_IP/"
echo "üöÄ OpenHands:       http://$SERVER_IP:3000"
echo "üí¨ Open WebUI:      http://$SERVER_IP:8080"
echo "ü§ñ Ollama API:      http://$SERVER_IP:11434"
echo ""
echo "=========================================="
echo "üìä Service Ports:"
echo "=========================================="
echo "Port 80:    Landing Page (Nginx)"
echo "Port 3000:  OpenHands Frontend"
echo "Port 3001:  OpenHands Backend"
echo "Port 8080:  Open WebUI"
echo "Port 11434: Ollama API"
echo "Port 30369: OpenHands Runtime"
echo ""
echo "=========================================="
echo "üîß Useful Commands:"
echo "=========================================="
echo "View logs:           docker-compose logs -f"
echo "Restart services:    docker-compose restart"
echo "Stop services:       docker-compose down"
echo "List models:         ollama list"
echo "Pull new model:      ollama pull <model-name>"
echo ""
echo "=========================================="
echo "üí° Tips:"
echo "=========================================="
echo "1. First-time Open WebUI users need to sign up"
echo "2. CodeLlama 70B requires ~70GB RAM for optimal performance"
echo "3. All services are accessible from outside the server"
echo "4. Firewall is configured to allow all necessary ports"
echo ""

# Show running containers
print_status "üê≥ Running containers:"
sudo docker ps

# Show system resources
print_status "üíª System resources:"
free -h
df -h /
